{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "seq = 12\n",
    "dt = 8  \n",
    "def multiply(A,B):\n",
    "    return [a*b for a,b in zip(A,B)]\n",
    "def divide(A,B):\n",
    "    return [a/b for a,b in zip(A,B)]\n",
    "\n",
    "def minus(A,B):\n",
    "    return [a-b for a,b in zip(A,B)]\n",
    "def log(txt):\n",
    "    f= open(\"log%i.txt\"%seq,\"a+\")\n",
    "    f.write('%s: %s \\r\\n'%(datetime.now().strftime('{%Y-%m-%d %H:%M:%S}'),txt))\n",
    "    print(txt)\n",
    "    f.close() \n",
    "def initial_log():\n",
    "    f= open(\"log%i.txt\"%seq,\"w+\")\n",
    "    f.write('%s: starting.... \\r\\n'%(datetime.now().strftime('{%Y-%m-%d %H:%M:%S}')))\n",
    "    f.close() \n",
    "import pickle\n",
    "\n",
    "def savedf():\n",
    "    with open('df%i.dat'%dt,'wb') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test),f)\n",
    "\n",
    "def loaddf():\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    with open('df%i.dat'%dt,'rb') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "    print(\"Shape of X_train:\", len(X_train),len(X_train[0]))\n",
    "    print(\"Shape of X_test:\",len(X_test),len(X_test[0]))\n",
    "\n",
    "      \n",
    "def save():\n",
    "    with open('data%i.dat'%seq,'wb') as f:\n",
    "        pickle.dump((i,mdl_1,mdl_2,mdl_3,RMSETest,RMSETrain,last_depth,last_rmse),f)\n",
    "def load():\n",
    "    global i,mdl_1,mdl_2,mdl_3,RMSETest,RMSETrain,last_depth,last_rmse\n",
    "    with open('data%i.dat'%seq,'rb') as f:\n",
    "        i,mdl_1,mdl_2,mdl_3,RMSETest,RMSETrain,last_depth,last_rmse = pickle.load(f)\n",
    "\n",
    "def load1():\n",
    "    global i,mdl_1,mdl_2,mdl_3,RMSETest,RMSETrain,last_depth,last_rmse\n",
    "    with open('data4.dat','rb') as f:\n",
    "        i,mdl_1,mdl_2,mdl_3,RMSETest,RMSETrain,last_depth,last_rmse = pickle.load(f)\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep +\"C:\\\\Users\\\\User\\\\Downloads\\\\graphviz-2.38\\\\release\\\\bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import var\n",
    "\n",
    "class Splitter:\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def split(self,index,value):\n",
    "        \"\"\" \n",
    "        Split X and Y based on the given variable index and value\n",
    "  \n",
    "        Parameters: \n",
    "            index (int): index of variable to splitted on\n",
    "            value (float): the value of splitting varaible\n",
    "          \n",
    "        Returns: \n",
    "            right_x,left_x,rigth_y,left_y\n",
    "        \"\"\"\n",
    "        right_x = [x for x in self.X if x[index]>=value]\n",
    "        left_x = [x for x in self.X if x[index]<value]\n",
    "        \n",
    "        right_y = [self.Y[i] for i,x in enumerate(self.X) if x[index]>=value]\n",
    "        left_y = [self.Y[i] for i,x in enumerate(self.X) if x[index]<value]\n",
    "        return right_x,left_x,right_y,left_y\n",
    "    \n",
    "    def weighted_var(self,Y):\n",
    "        if len(Y) ==0:\n",
    "            return float('inf')\n",
    "        return var(Y)/len(Y)\n",
    "    \n",
    "    def SE(self,Y):\n",
    "        if len(Y) ==0: return float('inf')\n",
    "        mean = sum(Y)/len(Y)\n",
    "        SE = sum([(y-mean)**2 for y in Y])\n",
    "        return SE\n",
    "        \n",
    "    \n",
    "    def splitting_cost(self,right_x,left_x,right_y,left_y):\n",
    "        \"\"\" \n",
    "        Compute cost of spliting.\n",
    "        in the current version, we have used varaince based cost function\n",
    "  \n",
    "        Parameters: \n",
    "            right_x,left_x,rigth_y,left_y\n",
    "          \n",
    "        Returns: \n",
    "            float: the cost of computing\n",
    "        \"\"\"        \n",
    "        return (self.SE(right_y)+self.SE(left_y))/(len(right_y)+len(left_y))\n",
    "    \n",
    "    def possibleValues(self,index):\n",
    "        values = [x[index] for x in self.X]\n",
    "        values.sort()\n",
    "        max_values = 100\n",
    "        if len(values)<max_values:\n",
    "            return values\n",
    "        \n",
    "        return set([np.percentile(values,i*100//max_values) for i in range(max_values)])\n",
    "    \n",
    "    def test_splitting(self,index,value):\n",
    "        right_x,left_x,right_y,left_y = self.split(index,value)\n",
    "        return  self.splitting_cost(right_x,left_x,right_y,left_y),len(right_y),len(left_y)\n",
    "    \n",
    "    def find_best_splitting(self,min_leaf_treshold=0,min_split=0):\n",
    "        \"\"\" \n",
    "        Find best splitting and return index and value of splitting variable respectively\n",
    "       \n",
    "        Returns: \n",
    "            int,float: the index and value of best possible splitting variables\n",
    "        \"\"\" \n",
    "        \n",
    "        best_splitting = (self.SE(self.Y)/len(self.Y))\n",
    "        best_splitting_index = -1\n",
    "        best_splitting_value = 0\n",
    "        if len(self.Y)<min_split:\n",
    "            return best_splitting_index,best_splitting_value            \n",
    "        for index in range(len(self.X[0])):\n",
    "            for value in self.possibleValues(index):\n",
    "                splitting_cost,left_cnt,right_cnt = self.test_splitting(index,value)\n",
    "#               print('splitting at %f: cost->%f'%(value,splitting_cost))\n",
    "                if splitting_cost < best_splitting and left_cnt >= min_leaf_treshold and right_cnt >= min_leaf_treshold:\n",
    "                    best_splitting = splitting_cost\n",
    "                    best_splitting_index = index\n",
    "                    best_splitting_value = value\n",
    "        return best_splitting_index,best_splitting_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'digraph {\\n\\tgraph [rankdir=UT]\\n\\t0 [label=\"f0<8.40779079e-45\"]\\n\\t0 -> 1 [label=\"yes, missing\" color=\"#0000FF\"]\\n\\t0 -> 2 [label=no color=\"#FF0000\"]\\n\\t1 [label=\"f0<2.80259693e-45\"]\\n\\t1 -> 3 [label=\"yes, missing\" color=\"#0000FF\"]\\n\\t1 -> 4 [label=no color=\"#FF0000\"]\\n\\t3 [label=\"leaf=7.46349096\"]\\n\\t4 [label=\"leaf=-0.278686523\"]\\n\\t2 [label=\"f0<8.40779079e-45\"]\\n\\t2 -> 5 [label=\"yes, missing\" color=\"#0000FF\"]\\n\\t2 -> 6 [label=no color=\"#FF0000\"]\\n\\t5 [label=\"leaf=-0\"]\\n\\t6 [label=\"leaf=-0.188493282\"]\\n}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 0\n",
    "last_leaf_no = 1\n",
    "j = 0\n",
    "def create_Tree():\n",
    "    global last_leaf_no\n",
    "    last_leaf_no = 1\n",
    "    return Node(0)\n",
    "class Node:\n",
    "    isLeaf = True\n",
    "    Left = None\n",
    "    Right = None\n",
    "    Value = None\n",
    "    Depth = 0\n",
    "    splitting_var_index = 0\n",
    "    splitting_value = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    \n",
    "    def __init__(self,depth):\n",
    "        global id\n",
    "        id+=1\n",
    "        self.ID = id\n",
    "        self.isLeaf = True\n",
    "        self.Depth = depth\n",
    "    def predict(self,X):\n",
    "        return [self.predict_one(x).Value for x in X]\n",
    "        \n",
    "    def predict_one(self,X):\n",
    "        if type(X) != list:\n",
    "            raise TypeError(\"predict_one.X: expected list but recieved %s (%s)\"%(X,type(X)))\n",
    "        try:\n",
    "            if self.isLeaf:\n",
    "                self.cnt +=1\n",
    "                return self\n",
    "            if X[self.splitting_var_index] >= self.splitting_value:\n",
    "                return self.Right.predict_one(X)\n",
    "            else:\n",
    "                return self.Left.predict_one(X)\n",
    "        except IndexError:\n",
    "            print('index: %i'%self.splitting_var_index)\n",
    "            raise\n",
    "    def get_leafs(self):\n",
    "        if self.isLeaf:\n",
    "            return [self]\n",
    "        return self.Right.get_leafs()+self.Left.get_leafs()   \n",
    "    \n",
    "    def fit(self,X,Y,max_depth):\n",
    "        global last_leaf_no\n",
    "\n",
    "        #tree reached maximum depth\n",
    "        if max_depth <= self.Depth:\n",
    "            self.isLeaf = True\n",
    "            self.leafID = last_leaf_no\n",
    "            last_leaf_no+=1\n",
    "            self.fit_leaf_value(Y)\n",
    "            return \n",
    "        \n",
    "        #continue splitting if more depth allowed\n",
    "        splitter = Splitter(X,Y)\n",
    "        best_splitting_index,best_splitting_value = splitter.find_best_splitting()\n",
    "        if best_splitting_index == -1:\n",
    "            #better to not split\n",
    "            self.isLeaf = True\n",
    "            self.leafID = last_leaf_no\n",
    "            last_leaf_no+=1\n",
    "            self.fit_leaf_value(Y)\n",
    "        else:\n",
    "            right_x,left_x,right_y,left_y = splitter.split(best_splitting_index,best_splitting_value)\n",
    "            self.isLeaf = False\n",
    "            self.splitting_var_index = best_splitting_index\n",
    "            self.splitting_value = best_splitting_value\n",
    "            self.Right = Node(self.Depth+1)\n",
    "            self.Left = Node(self.Depth+1)\n",
    "            self.Right.fit(right_x,right_y,max_depth)\n",
    "            self.Left.fit(left_x,left_y,max_depth)\n",
    "    def fit_leaf_value(self,Y):\n",
    "        self.Value = sum(Y)/len(Y)\n",
    "\n",
    "        \n",
    "    def tune(self,X,R,T):\n",
    "        #self.X = X\n",
    "       # self.Y = ['%f:%f'%(r,t) for r,t in zip(R,T)]\n",
    "        if len(X) != len(R):\n",
    "            raise TypeError(\"tune function: X and R should be with same size. recieved <%i>.<%i>\"%(len(X),len(R)))\n",
    "        if len(X) != len(T):\n",
    "            raise TypeError(\"tune function: X and T should be with same size. recieved <%i>.<%i>\"%(len(X),len(T)))\n",
    "            \n",
    "        if self.isLeaf:\n",
    "            if sum(multiply(T,T)) == 0:\n",
    "                self.value = 1\n",
    "            else:\n",
    "                self.Value = sum(multiply(R,T))/sum(multiply(T,T))\n",
    "            return\n",
    "        \n",
    "        splitter = Splitter(X,R)\n",
    "        right_x,left_X,right_R,left_R = splitter.split(self.splitting_var_index,self.splitting_value)\n",
    "        splitter = Splitter(X,T)\n",
    "        right_x,left_X,right_T,left_T = splitter.split(self.splitting_var_index,self.splitting_value)\n",
    "        self.Right.tune(right_x,right_R,right_T)\n",
    "        self.Left.tune(left_X,left_R,left_T)\n",
    "        \n",
    "    def draw_tree(self):\n",
    "        global j\n",
    "        from graphviz import Source\n",
    "        from io import BytesIO\n",
    "        import time\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        from matplotlib import pyplot as plt\n",
    "        from matplotlib import image\n",
    "        g = Source('digraph {\\n\\tgraph [rankdir=UT]\\n'+self.print_tree()+'}')\n",
    "        g.render('round-table%i.gv'%j, view=True)\n",
    "        j+=1\n",
    "        #return img\n",
    "        \n",
    "    def get_leafcounts(self):\n",
    "        if self.isLeaf:\n",
    "            return 1\n",
    "        return self.Left.get_leafcounts()+self.Right.get_leafcounts()\n",
    "    \n",
    "    def print_tree(self,prefix=''):\n",
    "        '\\t0 [label=\"f0<8.40779079e-45] yes=1,no=2,missing=1\\n\\t1:[f0<2.80259693e-45] yes=3,no=4,missing=3\\n\\t\\t3:leaf=7.46349096\\n\\t\\t4:leaf=-0.278686523\\n\\t2:[f0<8.40779079e-45] yes=5,no=6,missing=5\\n\\t\\t5:leaf=-0\\n\\t\\t6:leaf=-0.188493282\\n'\n",
    "        if not self.isLeaf:\n",
    "            ret = '\\t%i [label=\"f%i>=%f\"]\\n\\t%i -> %i [label=\"yes\" color=\"#0000FF\"]\\n\\t %i -> %i [label=no color=\"#FF0000\"]\\n'%(\n",
    "                self.ID,self.splitting_var_index,self.splitting_value,self.ID,self.Right.ID,self.ID,self.Left.ID)\n",
    "            ret += self.Right.print_tree(prefix+'\\t')\n",
    "            ret += self.Left.print_tree(prefix+'\\t')\n",
    "            return ret\n",
    "        else:\n",
    "            return '\\t%i [label=\"leaf=%f\\nCount=%i\"]\\n'%(self.ID,self.Value,self.cnt)\n",
    "    def get_leaf_values(self):\n",
    "        if self.isLeaf:\n",
    "            return [self.Value]\n",
    "        return self.Right.get_leaf_values()+self.Left.get_leaf_values()\n",
    "    \n",
    "'''digraph {\n",
    "\tgraph [rankdir=UT]\n",
    "\t0 [label=\"f0<8.40779079e-45\"]\n",
    "\t0 -> 1 [label=\"yes, missing\" color=\"#0000FF\"]\n",
    "\t0 -> 2 [label=no color=\"#FF0000\"]\n",
    "\t1 [label=\"f0<2.80259693e-45\"]\n",
    "\t1 -> 3 [label=\"yes, missing\" color=\"#0000FF\"]\n",
    "\t1 -> 4 [label=no color=\"#FF0000\"]\n",
    "\t3 [label=\"leaf=7.46349096\"]\n",
    "\t4 [label=\"leaf=-0.278686523\"]\n",
    "\t2 [label=\"f0<8.40779079e-45\"]\n",
    "\t2 -> 5 [label=\"yes, missing\" color=\"#0000FF\"]\n",
    "\t2 -> 6 [label=no color=\"#FF0000\"]\n",
    "\t5 [label=\"leaf=-0\"]\n",
    "\t6 [label=\"leaf=-0.188493282\"]\n",
    "}'''       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from copy import deepcopy\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from scipy import sparse\n",
    "    import numpy as np\n",
    "    from scipy.optimize import lsq_linear\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    class FractBoosting:\n",
    "\n",
    "        def __init__(self,learning_rate):\n",
    "            self.learning_rate = learning_rate\n",
    "            self.trees = []\n",
    "\n",
    "        def predict(self,X,ignore_last=0):\n",
    "            Y = []\n",
    "            tree_filtered = self.trees[:max(1,len(self.trees)-ignore_last)]\n",
    "            print('prediciting using %i trees'%len(tree_filtered))\n",
    "            for x in X:\n",
    "                predictions = [S.predict([x])[0]*T.predict([x])[0] for S,T in tree_filtered]\n",
    "                predict = predictions[0] + self.learning_rate*sum(predictions[1:])\n",
    "                Y.append(predict)\n",
    "\n",
    "            return Y\n",
    "\n",
    "        def fit(self,X,Y,max_depth,max_trees):\n",
    "            \"\"\" \n",
    "            Fit the boosting trees on the given data\n",
    "\n",
    "            Parameters: \n",
    "                X (list of list): the input X\n",
    "                Y (list): input Y\n",
    "                max_depth (int): maximum depth of trees\n",
    "                max_trees (int): the number of trees\n",
    "            \"\"\" \n",
    "\n",
    "            lastRMSE = self.rmse(X,Y)\n",
    "            improvment = []\n",
    "            for i in range(1,max_trees):\n",
    "                t0 = time()\n",
    "                depth = int(1+max_depth*sqrt(i/max_trees))\n",
    "                log('starting itteration %i with depth %i'%(i,depth))\n",
    "    #            print('itteration %i, begining, mean loss: %f, tree depth: %i'%(i,self.rmse(X,Y),depth))\n",
    "                self.fit_one_step(X,Y,depth)\n",
    "\n",
    "                current_rmse = self.rmse(X,Y)\n",
    "                RMSEDif = lastRMSE - current_rmse\n",
    "                lastRMSE = current_rmse\n",
    "                cost = T.get_leafcounts()\n",
    "                imp = cost/RMSEDif\n",
    "                improvment.append(imp)\n",
    "                log('itteration %i done in %0.3fs, RMSE: %f, Cost/improvement: %f'%(i,time() - t0,current_rmse,imp))\n",
    "            return improvment\n",
    "        def create_root_tree(self,Y):\n",
    "            root_s = create_Tree()\n",
    "            root_s.Value = 1\n",
    "            root_t = create_Tree()\n",
    "            root_t.fit_leaf_value(Y)\n",
    "            self.trees = [(root_s,root_t)]\n",
    "\n",
    "        def fit_one_step(self,X_TRAIN,Y_TRAIN,depth,backprop_level):\n",
    "            X, X1, Y, Y1 = train_test_split(X_TRAIN, Y_TRAIN, test_size=0.2,shuffle=False)\n",
    "            Y_r = self.get_residual(X,Y)\n",
    "            T = create_Tree()\n",
    "            T.fit(X,Y_r,depth)\n",
    "            Y_t = T.predict(X)\n",
    "            S = deepcopy(self.trees[-1][1])\n",
    "            S.tune(X,Y_r,Y_t)\n",
    "            self.trees.append((S,T))\n",
    "            X, _, Y, _ = train_test_split(X_TRAIN, Y_TRAIN, test_size=0.8,shuffle=False)\n",
    "            X = X+X1\n",
    "            Y = Y+Y1\n",
    "            self.back_prop(X,Y,backprop_level)\n",
    "            \n",
    "        def back_prop(self,X,Y,level):\n",
    "            global A4\n",
    "            print(len(X),len(Y))\n",
    "            #self.trees[-1][0].draw_tree()\n",
    "            h = self.predict(X,level)\n",
    "            R = np.array(minus(Y , h))\n",
    "            tr = self.trees[max(1,len(self.trees)-level):]\n",
    "            print('computing last %i trees of S [total trees: %i]'%(len(tr),len(self.trees)))\n",
    "            i = 0\n",
    "            S_nodes =[]\n",
    "            s_values = []\n",
    "            for s,_ in tr:\n",
    "                for leaf in s.get_leafs():\n",
    "                    leaf.leaf_index = i\n",
    "                    i+=1\n",
    "                    S_nodes.append(leaf)\n",
    "                    s_values.append(leaf.Value)\n",
    "            row = []\n",
    "            col = []\n",
    "            data =[]\n",
    "            for i,x in enumerate(X):\n",
    "                for s,t in tr:\n",
    "                    s_leaf = s.predict_one(x)\n",
    "                    t_leaf = t.predict_one(x)\n",
    "                    row.append(i)\n",
    "                    col.append(s_leaf.leaf_index)\n",
    "                    data.append(t_leaf.Value)\n",
    "            A = sparse.csr_matrix((data, (row, col)), shape=(len(X), len(S_nodes))) \n",
    "            ##U = A.transpose().dot(A).todense()\n",
    "            #V = A.transpose().dot(R)\n",
    "            #A4 = np.linalg.pinv(U,hermitian =True).dot(V)\n",
    "            #print(A4.shape)\n",
    "            A4 = lsq_linear(A,R, bounds=(-0.01, .1)).x\n",
    "            print(\"S Values before \",s_values)\n",
    "            print('S Values after ',A4)\n",
    "            #A4 = A4.todesne()\n",
    "#            A1 = A.transpose().dot(A)\n",
    "#            print(np.linalg.matrix_rank(A.todense()),np.linalg.matrix_rank(A1.todense()))\n",
    "#            A2 = sparse.csr_matrix(inv(A1))\n",
    "#            A3 = A.transpose().dot(R)\n",
    "#            A4 = (A2.dot(A3)).todense()\n",
    "            print('rmse before:',self.rmse(X,Y))\n",
    "            r1 = self.get_residual(X,Y)\n",
    "            for i,s1 in enumerate(S_nodes):\n",
    "                s1.Value = A4[i]\n",
    "            #self.trees[-1][0].draw_tree()\n",
    "            print('rmse after:',self.rmse(X,Y))\n",
    "            r2 = self.get_residual(X,Y)\n",
    "            #print('Residual before ',r1)\n",
    "            #print('Residual after ',r2)\n",
    "            #print('diff ',minus(r1,r2))\n",
    "            \n",
    "        def get_residual(self,X,Y):\n",
    "            h = self.predict(X)\n",
    "            Y_r = minus(Y , h)\n",
    "            return Y_r\n",
    "        def print_trees(self):\n",
    "            for i,(s,t) in enumerate(self.trees):\n",
    "                print('%ith fractal tree\\n'%i)\n",
    "                print('-----------------\\n')\n",
    "                print('S:\\n')\n",
    "                s.print_tree()\n",
    "                print('-----------------\\n')\n",
    "                print('T:\\n')\n",
    "                t.print_tree()\n",
    "                print('\\n-----------------\\n')\n",
    "\n",
    "        def rmse(self,X,Y):\n",
    "            h = self.predict(X)\n",
    "    #        R = minus(Y , h)\n",
    "    #        print('Residual:',R)\n",
    "            return sqrt(mean_squared_error(h, Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import std\n",
    "import xgboost as xgb\n",
    "from threading import Thread\n",
    "from xgboost import plot_tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "max_depth=5\n",
    "\n",
    "loaddf()\n",
    "#initial_log()\n",
    "mdl_1 = FractBoosting(1)\n",
    "mdl_2 = FractBoosting(1)\n",
    "mdl_3 = FractBoosting(1)\n",
    "mdl_1.create_root_tree(y_train)\n",
    "mdl_2.create_root_tree(y_train)\n",
    "mdl_3.create_root_tree(y_train)\n",
    "last_rmse = mdl_3.rmse(X_train,y_train)\n",
    "last_depth = 1\n",
    "\n",
    "m = sum(y_train)/len(y_train)\n",
    "y_train_xgb = [y-m for y in y_train]\n",
    "\n",
    "RMSETest = [[],[],[],[],[],[]]\n",
    "RMSETrain =  [[],[],[],[],[],[]]\n",
    "i=0\n",
    "try:\n",
    "    load()\n",
    "    i\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    initial_log()\n",
    "try:\n",
    "    while i<200:\n",
    "        processes = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ################Fixed depth\n",
    "        mdl_1.fit_one_step(X_train,y_train,max_depth,1)\n",
    "        RMSETrain[0].append( mdl_1.rmse(X_train,y_train))\n",
    "        RMSETest[0].append( mdl_1.rmse(X_test,y_test))    \n",
    "        print('1 Back')\n",
    "\n",
    "\n",
    "        \n",
    "        ###############Linear depth\n",
    "        #depth = min(5,int(1+max_depth*(i/50)))\n",
    "        mdl_2.fit_one_step(X_train,y_train,max_depth,10)  \n",
    "        RMSETrain[1].append( mdl_2.rmse(X_train,y_train))\n",
    "        RMSETest[1].append( mdl_2.rmse(X_test,y_test))    \n",
    "        print('2 Back')\n",
    "\n",
    "\n",
    "        ###############dynamic depth\n",
    "        mdl_3.fit_one_step(X_train,y_train,max_depth,20)\n",
    "        #mdl3_rmse = mdl_3.rmse(X_test,y_test)\n",
    "        RMSETrain[2].append( mdl_3.rmse(X_train,y_train))\n",
    "        RMSETest[2].append( mdl_3.rmse(X_test,y_test))\n",
    "        print('3 Back')\n",
    "        \n",
    "\n",
    "\n",
    "        ##################XGBoost\n",
    "        xg_reg = xgb.XGBRegressor( learning_rate = 0.1,\n",
    "                        max_depth = 5,  n_estimators = i)\n",
    "        xg_reg.fit(X_train,y_train_xgb)    \n",
    "        preds = xg_reg.predict(X_train)\n",
    "        RMSETrain[3].append(np.sqrt(mean_squared_error(y_train, [p+m for p in preds])))\n",
    "        preds = xg_reg.predict(X_test)\n",
    "        RMSETest[3].append(np.sqrt(mean_squared_error(y_test, [p+m for p in preds])))\n",
    "        print('XGB')\n",
    "        i+=1\n",
    "        \n",
    "        save()\n",
    "                           \n",
    "        log('\\r\\n-----------------------------------------------\\r\\n'+\n",
    "            'ittr %i  \\tTrain\\t\\tTest\\t\\tDiff\\t\\tRVar\\tSVar\\tTVar\\r\\n'%i+\n",
    "            'FGB[%i]:  \\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t%.3f\\t\\t%.3f\\r\\n'%(1,RMSETrain[0][-1],RMSETest[0][-1],RMSETest[0][-1]-RMSETrain[0][-1],std(mdl_1.get_residual(X_train,y_train)),std(mdl_1.trees[-1][0].get_leaf_values()),std(mdl_1.trees[-1][1].get_leaf_values()))+\n",
    "            'FGB[%i]:  \\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t%.3f\\t\\t%.3f\\r\\n'%(10,RMSETrain[1][-1],RMSETest[1][-1],RMSETest[1][-1]-RMSETrain[1][-1],std(mdl_2.get_residual(X_train,y_train)),std(mdl_2.trees[-1][0].get_leaf_values()),std(mdl_2.trees[-1][1].get_leaf_values()))+\n",
    "            'FGB[%i]:  \\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t%.3f\\t\\t%.3f\\r\\n'%(20,RMSETrain[2][-1],RMSETest[2][-1],RMSETest[2][-1]-RMSETrain[2][-1],std(mdl_3.get_residual(X_train,y_train)),std(mdl_3.trees[-1][0].get_leaf_values()),std(mdl_3.trees[-1][1].get_leaf_values()))+\n",
    "            'XGB[%i]:  \\t%.3f\\t\\t%.3f\\t\\t%.3f \\r\\n'%(max_depth,RMSETrain[3][-1],RMSETest[3][-1],RMSETest[3][-1]-RMSETrain[3][-1]))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    log(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [7], [8], [3], [4], [10], [6], [8], [5], [3]]\n",
      "prediciting using 2 trees\n",
      "[0.0, 0.0, 3.333333333333333, 1.0, 0.0, 1.333333333333333, 1.0, -4.666666666666667, 0.0, -2.0]\n",
      "prediciting using 2 trees\n",
      "[1] 0.0\n",
      "[7] 0.0\n",
      "[3] 1.0\n",
      "[4] 0.0\n",
      "[6] 1.0\n",
      "[5] 0.0\n",
      "[3] -2.0\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(minus(Y,mdl.predict(X,1)))\n",
    "for x,y in zip(X,minus(Y,mdl.predict(X,1))):\n",
    "    if x[0]<8:\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CASP.csv', <http.client.HTTPMessage at 0x1b6360ea088>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib# url of the data\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv'\n",
    "# retrive data from the url and store in the folder name Data with filename as CASP.csv\n",
    "urllib.request.urlretrieve (url, filename=\"CASP.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: 22865 9\n",
      "Shape of X_test: 22865 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "# Stratified sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "df = pd.read_csv(\"CASP.csv\")\n",
    "labels = [1,2,3,4,5]\n",
    "df['RMSD_cat'] = pd.cut(df['RMSD'], 5, labels=labels)\n",
    "df.head()\n",
    "# Create the instance from object\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
    "\n",
    "# Create Train - Test Set\n",
    "for train_index, test_index in split.split(df, df[\"RMSD_cat\"]):\n",
    "    train_set = df.loc[train_index]\n",
    "    test_set = df.loc[test_index]\n",
    "\n",
    "# Removing the RMSD_cat to give the original data\n",
    "for _set in (train_set, test_set):\n",
    "    _set.drop([\"RMSD_cat\"], axis=1, inplace=True)\n",
    "    \n",
    "# Creating new copies of Stratified Train and Test data\n",
    "y_train = train_set[\"RMSD\"].copy().values.tolist()\n",
    "X_train = train_set.drop(\"RMSD\", axis=1).values.tolist()\n",
    "\n",
    "y_test = test_set[\"RMSD\"].copy().values.tolist()\n",
    "X_test = test_set.drop(\"RMSD\", axis=1).values.tolist()\n",
    "print(\"Shape of X_train:\", len(X_train),len(X_train[0]))\n",
    "print(\"Shape of X_test:\",len(X_test),len(X_test[0]))\n",
    "savedf()\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeXY(node):\n",
    "    if node == None:\n",
    "        return\n",
    "    node.X = node.Y = ''\n",
    "    removeXY(node.Left)\n",
    "    removeXY(node.Right)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(500, 500))\n",
    "removeXY(mdl.trees[85][0])\n",
    "mdl.trees[85][0].draw_tree(ax=ax)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
